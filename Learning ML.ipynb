{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120cd118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "\n",
    "# See TensorFlow version: working on tensorflow-macos: 2.9.0, tensorflow-metal: 0.5.0 (https://developer.apple.com/metal/tensorflow-plugin/)\n",
    "print(f\"TensorFlow version: {tf.__version__}\") \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 \n",
    "\n",
    "import tensorflow_datasets as tfds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "model = Sequential([Dense(units=1, input_shape=[1])])\n",
    "# only one line in Sequential so our neural network consists of only one layer\n",
    "# many different layer types. 'Dense' means a set of fully connected neurons (most common)\n",
    "# 'units=1' means only one neuron in entire neural network\n",
    "# input data is only X here so 'input_shape=[1]' \n",
    "\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "# sgd (stochastic gradient descent) - mathematical function that, when given the values, the previous guess, and\n",
    "# the results of calculating the errors (or loss) on that guess, can then generate another one. Over time, its\n",
    "# job is to minimize the loss, and by doing so bring the guessed formula closer and closer to the correct answer.\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model.fit(xs, ys, epochs=500) \n",
    "# epochs \"number of passes a training dataset takes around an algorithm\" -simplilearn\n",
    "# read it as 'fit the Xs to the Ys, and try it 500 times'\n",
    "\n",
    "print(model.predict([10.0]))\n",
    "# run prediction using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "l0 = Dense(units=1, input_shape=[1])\n",
    "model = Sequential([l0])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model.fit(xs, ys, epochs=500) \n",
    "\n",
    "print(model.predict([10.0]))\n",
    "\n",
    "print(\"Here is what I learned: {}\".format(l0.get_weights()))\n",
    "# neuron learns a weight and bias (Y = WX + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae7c79",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1929e41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# loading fashion MNIST data\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "# normalizing the image - ensures every pixel is represented by a number between 0 and 1\n",
    "# normalizing is important to ensure the model is trained well\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "# line 1: input layer specification: flatten takes the 2d array and turns into a line - 1d array\n",
    "# line 2: middle layer (hidden layer): layer of 128 neurons\n",
    "# line 3 output layer : 10 neurons as we have 10 classes\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels) # test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f146004",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = model.predict(test_images) \n",
    "print(classifications[1]) \n",
    "print(test_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6eb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# loading fashion MNIST data\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "# normalizing the image - ensures every pixel is represented by a number between 0 and 1\n",
    "# normalizing is important to ensure the model is trained well\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "# line 1: input layer specification: flatten takes the 2d array and turns into a line - 1d array\n",
    "# line 1: (28, 28) as image size is 28 * 28\n",
    "# line 2: middle layer (hidden layer): layer of 128 neurons\n",
    "# line 3 output layer : 10 neurons as we have 10 classes\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=50) # discovering overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels) # test on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a65a16",
   "metadata": {},
   "source": [
    "### Automatically stopping training after reaching a particular accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fa782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback): \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.95):\n",
    "            print(\"\\nReached 95% accuracy so cancelling training!\") \n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callbacks = myCallback()\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe8930",
   "metadata": {},
   "source": [
    "## Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946baf6",
   "metadata": {},
   "source": [
    "### Adding a Convolutional Neural Network (CNN) to Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist \n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images = training_images.reshape(60000, 28, 28, 1) # input shape needs to match Conv2D layer- 60,000 images\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "# line 1: '64': number of convolutions (will learn which is the best), (3, 3): is the size of the filter\n",
    "# line 1: (28, 28, 1): 28 * 28 image (same as before), Conv2D layers designed for RGB, our dataset is monochrome (1)\n",
    "# line 2: pooling layer in NN. 2 * 2 pool.\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=20)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images) \n",
    "print(classifications[0]) \n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c57c7",
   "metadata": {},
   "source": [
    "### Building a CNN to Distinguish Between Horses and Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
    "\n",
    "file_name = \"horse-or-human.zip\"\n",
    "training_dir = 'horse-or-human/training/'\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall(training_dir)\n",
    "zip_ref.close()\n",
    "# downloading the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(300, 300), # size of image\n",
    "    class_mode='binary' # binary if two kinds of images, categorical if more than 2\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "      tf.keras.layers.MaxPooling2D(2, 2),\n",
    "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(512, activation='relu'),\n",
    "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# more layers as image is much bigger than MNIST. (300*300)\n",
    "# (300, 300, 3) full color so 3.\n",
    "# last layer has one neuron as it is a binary classifier. \n",
    "# sigmoid function: to drive one set of values to 0 and other toward 1\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit_generator(\n",
    "#       train_generator,\n",
    "#       epochs=15\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
    "\n",
    "validation_file_name = \"validation-horse-or-human.zip\"\n",
    "validation_dir = 'horse-or-human/validation/'\n",
    "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
    "zip_ref.extractall(validation_dir)\n",
    "zip_ref.close()\n",
    "# downloading the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(300, 300),\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9894a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1022e12f",
   "metadata": {},
   "source": [
    "### Testing the dataset with other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/boy-1284509_1920.jpg'\n",
    "\n",
    "# displaying the image\n",
    "img = mpimg.imread(path)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(300,300))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "if classes[0] > 0.5:\n",
    "    print(\"the image is of a human\")\n",
    "else:\n",
    "    print(\"the image is of a horse\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/horse-1330690_1920.jpg'\n",
    "\n",
    "img1 = mpimg.imread(path)\n",
    "imgplot = plt.imshow(img1)\n",
    "plt.show()\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(300,300))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"the image is of a human\")\n",
    "else:\n",
    "    print(\"the image is of a horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/business-1287044_1920.jpg'\n",
    "\n",
    "img1 = mpimg.imread(path)\n",
    "imgplot = plt.imshow(img1)\n",
    "plt.show()\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(300,300))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"the image is of a human\")\n",
    "else:\n",
    "    print(\"the image is of a horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/out-0.jpeg'\n",
    "\n",
    "img1 = mpimg.imread(path)\n",
    "imgplot = plt.imshow(img1)\n",
    "plt.show()\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(300,300))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"the image is of a human\")\n",
    "else:\n",
    "    print(\"the image is of a horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/icelandic-horse-1178169612-bb949fbfde104e5182b30c829d57de17.jpg'\n",
    "\n",
    "img1 = mpimg.imread(path)\n",
    "imgplot = plt.imshow(img1)\n",
    "plt.show()\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(300,300))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor)\n",
    "\n",
    "print(classes)\n",
    "# print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"the image is of a human\")\n",
    "else:\n",
    "    print(\"the image is of a horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d9a35",
   "metadata": {},
   "source": [
    "### Training with Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# The following lines will rescale, rotate, shift horizontallya and vertically, shear, zoom, flip.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(300, 300), # size of image\n",
    "    # batch_size=128,\n",
    "    class_mode='binary' # binary if two kinds of images, categorical if more than 2\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "      tf.keras.layers.MaxPooling2D(2, 2),\n",
    "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(2,2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(512, activation='relu'),\n",
    "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# more layers as image is much bigger than MNIST. (300*300)\n",
    "# (300, 300, 3) full color so 3.\n",
    "# last layer has one neuron as it is a binary classifier. \n",
    "# sigmoid function: to drive one set of values to 0 and other toward 1\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(300, 300),\n",
    "    # batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d14f7e",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 \n",
    "\n",
    "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "weights_file = \"inception_v3.h5\"\n",
    "urllib.request.urlretrieve(weights_url, weights_file)\n",
    "\n",
    "pre_trained_model = InceptionV3(input_shape=(150, 150, 3), include_top=False, weights=None)\n",
    "\n",
    "pre_trained_model.load_weights(weights_file)\n",
    "\n",
    "#pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a003aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers: \n",
    "    layer.trainable = False\n",
    "    \n",
    "last_layer = pre_trained_model.get_layer('mixed7') \n",
    "print('last layer output shape: ', last_layer.output_shape) \n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66194837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = tf.keras.layers.Flatten()(last_output)\n",
    "# Add a fully connected layer with 1,024 hidden units and ReLU activation \n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "# Add a dropout rate of 0.2\n",
    "x = tf.keras.layers.Dropout(0.2)(x)    \n",
    "# Add a final sigmoid layer for classification\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef544010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(pre_trained_model.input, x)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir = 'horse-or-human/validation'\n",
    "training_dir = 'horse-or-human/training'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(150, 150), # size of image\n",
    "    batch_size=20,\n",
    "    class_mode='binary' # binary if two kinds of images, categorical if more than 2\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/business-1287044_1920.jpg'\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(150,150))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x/255\n",
    "\n",
    "plt.imshow(tf.keras.utils.load_img(path))\n",
    "plt.show()\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor, batch_size=10)\n",
    "\n",
    "print(classes)\n",
    "print(classes[0])\n",
    "\n",
    "if classes[0] > 0.5:\n",
    "    print(\"the image is of a human\")\n",
    "else:\n",
    "    print(\"the image is of a horse\")\n",
    "    \n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9ab41",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://storage.googleapis.com/learning-datasets/rps.zip\"\n",
    "\n",
    "file_name = \"rps.zip\"\n",
    "training_dir = 'rps/'\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall('./')\n",
    "zip_ref.close()\n",
    "# downloading the images\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef439f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/learning-datasets/rps-test-set.zip\"\n",
    "\n",
    "file_name = \"rps-test-set.zip\"\n",
    "validation_dir = 'rps-test-set/'\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall('./')\n",
    "zip_ref.close()\n",
    "# downloading the images\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # Note the input shape is the desired size of the image: \n",
    "    # 150x150 with 3 bytes color\n",
    "    # This is the first convolution \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)), \n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Flatten the results to feed into a DNN \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, \n",
    "                    epochs=25, \n",
    "                    validation_data = validation_generator, \n",
    "                    verbose = 1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d270b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "path = 'test/rock07-k03-103.png'\n",
    "\n",
    "img = tf.keras.utils.load_img(path, target_size=(150,150))\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x/255\n",
    "\n",
    "plt.imshow(tf.keras.utils.load_img(path))\n",
    "plt.show()\n",
    "\n",
    "image_tensor = np.vstack([x])\n",
    "classes = model.predict(image_tensor, batch_size=10)\n",
    "\n",
    "print(classes)\n",
    "    \n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d10249",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduces chances of neurons becoming overspecialized\n",
    "tf.keras.layers.Dropout(0.2) \n",
    "# this will randomly drop out the specified percentage of neurons in the specified layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea56c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of use:\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)), \n",
    "    \n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c69160",
   "metadata": {},
   "source": [
    "Dropouts help in recognizing overfitting and removes ambiguity by ensuring the network isn't overspecializing to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa99347",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e3cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "mnist_data = tfds.load(\"fashion_mnist\") \n",
    "\n",
    "for item in mnist_data:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = tfds.load(name=\"fashion_mnist\", split=\"train\") \n",
    "assert isinstance(mnist_train, tf.data.Dataset) \n",
    "print(type(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in mnist_train.take(1): \n",
    "    print(type(item)) \n",
    "    print(item.keys())\n",
    "    print(item['image'])\n",
    "    print(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test, info = tfds.load(name=\"fashion_mnist\", with_info=\"true\") \n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74031f",
   "metadata": {},
   "source": [
    "### Fashion MNIST example with TFDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343135d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = tfds.as_numpy(tfds.load('fashion_mnist', \n",
    "                                                                                         split = ['train', 'test'],\n",
    "                                                                                         batch_size=-1,\n",
    "                                                                                         as_supervised=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd34657",
   "metadata": {},
   "source": [
    "### Horses or Human with TFDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e325396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True)\n",
    "\n",
    "train_batches = data.shuffle(100).batch(10)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu',\n",
    "                           input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_batches, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5649cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:13:58.583574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - ETA: 0s - loss: 2.7952 - accuracy: 0.8306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:05.393050: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 7s 66ms/step - loss: 2.7952 - accuracy: 0.8306 - val_loss: 0.7465 - val_accuracy: 0.8125\n",
      "Epoch 2/10\n",
      "  3/103 [..............................] - ETA: 5s - loss: 0.0563 - accuracy: 0.9667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:05.631585: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 61ms/step - loss: 0.0986 - accuracy: 0.9611 - val_loss: 0.3693 - val_accuracy: 0.9062\n",
      "Epoch 3/10\n",
      "  3/103 [..............................] - ETA: 5s - loss: 0.0127 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:11.939279: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 58ms/step - loss: 0.0766 - accuracy: 0.9708 - val_loss: 0.7293 - val_accuracy: 0.9062\n",
      "Epoch 4/10\n",
      "  3/103 [..............................] - ETA: 5s - loss: 0.0063 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:17.939690: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 58ms/step - loss: 0.0119 - accuracy: 0.9951 - val_loss: 0.8134 - val_accuracy: 0.9062\n",
      "Epoch 5/10\n",
      "  3/103 [..............................] - ETA: 6s - loss: 0.0014 - accuracy: 1.0000  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:23.931400: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 62ms/step - loss: 0.0679 - accuracy: 0.9737 - val_loss: 0.8684 - val_accuracy: 0.9062\n",
      "Epoch 6/10\n",
      "  3/103 [..............................] - ETA: 6s - loss: 0.0363 - accuracy: 0.9667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:30.271408: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 58ms/step - loss: 0.0590 - accuracy: 0.9825 - val_loss: 0.3883 - val_accuracy: 0.9375\n",
      "Epoch 7/10\n",
      "  3/103 [..............................] - ETA: 6s - loss: 0.0016 - accuracy: 1.0000  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:36.217863: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 57ms/step - loss: 0.0143 - accuracy: 0.9942 - val_loss: 1.6440 - val_accuracy: 0.8750\n",
      "Epoch 8/10\n",
      "  3/103 [..............................] - ETA: 6s - loss: 1.5314e-04 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:42.149888: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 60ms/step - loss: 7.7279e-04 - accuracy: 1.0000 - val_loss: 1.7696 - val_accuracy: 0.8750\n",
      "Epoch 9/10\n",
      "  2/103 [..............................] - ETA: 7s - loss: 1.7531e-04 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:48.304476: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 59ms/step - loss: 2.9838e-04 - accuracy: 1.0000 - val_loss: 1.8894 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "  2/103 [..............................] - ETA: 8s - loss: 6.2051e-05 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:14:54.369102: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 59ms/step - loss: 1.7910e-04 - accuracy: 1.0000 - val_loss: 1.9973 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 22:15:00.419187: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "val_data = tfds.load('horses_or_humans', split='test', as_supervised=True)\n",
    "validation_batches = val_data.batch(32)\n",
    "history = model.fit(train_batches, \n",
    "                    epochs=10, \n",
    "                    validation_data=validation_batches, \n",
    "                    validation_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25990704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
